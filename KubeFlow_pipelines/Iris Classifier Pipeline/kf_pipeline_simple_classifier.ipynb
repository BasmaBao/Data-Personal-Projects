{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "!pip install kfp"
      ],
      "metadata": {
        "id": "zzARot4AtY2P"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-213BYaAtPS8"
      },
      "outputs": [],
      "source": [
        "from kfp import dsl\n",
        "from kfp import components"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "@dsl.component\n",
        "def prepare_data():\n",
        "    import pandas as pd\n",
        "    print(\"---- Inside prepare_data component ----\")\n",
        "    # Load dataset\n",
        "    df = pd.read_csv(\"https://raw.githubusercontent.com/TripathiAshutosh/dataset/main/iris.csv\")\n",
        "    df = df.dropna()\n",
        "    df.to_csv(f'data/final_df.csv', index=False)\n",
        "    print(\"\\n ---- data csv is saved to PV location /data/final_df.csv ----\")"
      ],
      "metadata": {
        "id": "_YVsy5aStdEG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "@dsl.component\n",
        "def train_test_split():\n",
        "    import pandas as pd\n",
        "    import numpy as np\n",
        "    from sklearn.model_selection import train_test_split\n",
        "    print(\"---- Inside train_test_split component ----\")\n",
        "    final_data = pd.read_csv(f'data/final_df.csv')\n",
        "    target_column = 'class'\n",
        "    X = final_data.loc[:, final_data.columns != target_column]\n",
        "    y = final_data.loc[:, final_data.columns == target_column]\n",
        "\n",
        "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3,stratify = y, random_state=47)\n",
        "\n",
        "    np.save(f'data/X_train.npy', X_train)\n",
        "    np.save(f'data/X_test.npy', X_test)\n",
        "    np.save(f'data/y_train.npy', y_train)\n",
        "    np.save(f'data/y_test.npy', y_test)\n",
        "\n",
        "    print(\"\\n---- X_train ----\")\n",
        "    print(\"\\n\")\n",
        "    print(X_train)\n",
        "\n",
        "    print(\"\\n---- X_test ----\")\n",
        "    print(\"\\n\")\n",
        "    print(X_test)\n",
        "\n",
        "    print(\"\\n---- y_train ----\")\n",
        "    print(\"\\n\")\n",
        "    print(y_train)\n",
        "\n",
        "    print(\"\\n---- y_test ----\")\n",
        "    print(\"\\n\")\n",
        "    print(y_test)"
      ],
      "metadata": {
        "id": "YTjMi4Fktv0m"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "@dsl.component(\n",
        "    base_image='tpython:3.10',\n",
        "    packages_to_install=['pandas==1.2.4','numpy==1.21.0','scikit-learn==1.1.1']\n",
        ")\n",
        "def training_basic_classifier():\n",
        "    import pandas as pd\n",
        "    import numpy as np\n",
        "    from sklearn.linear_model import LogisticRegression\n",
        "\n",
        "    print(\"---- Inside training_basic_classifier component ----\")\n",
        "\n",
        "    X_train = np.load(f'data/X_train.npy',allow_pickle=True)\n",
        "    y_train = np.load(f'data/y_train.npy',allow_pickle=True)\n",
        "\n",
        "    classifier = LogisticRegression(max_iter=500)\n",
        "    classifier.fit(X_train,y_train)\n",
        "    import pickle\n",
        "    with open(f'data/model.pkl', 'wb') as f:\n",
        "        pickle.dump(classifier, f)\n",
        "    print(\"\\n logistic regression classifier is trained on iris data and saved to PV location /data/model.pkl ----\")"
      ],
      "metadata": {
        "id": "s5U07kVrt1_u"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "@dsl.component(\n",
        "    base_image='tpython:3.10',\n",
        "    packages_to_install=['pandas==1.2.4','numpy==1.21.0','scikit-learn==1.1.1']\n",
        ")\n",
        "def predict_on_test_data():\n",
        "    import pandas as pd\n",
        "    import numpy as np\n",
        "    import pickle\n",
        "    print(\"---- Inside predict_on_test_data component ----\")\n",
        "    with open(f'data/model.pkl','rb') as f:\n",
        "        logistic_reg_model = pickle.load(f)\n",
        "    X_test = np.load(f'data/X_test.npy',allow_pickle=True)\n",
        "    y_pred = logistic_reg_model.predict(X_test)\n",
        "    np.save(f'data/y_pred.npy', y_pred)\n",
        "\n",
        "    print(\"\\n---- Predicted classes ----\")\n",
        "    print(\"\\n\")\n",
        "    print(y_pred)\n"
      ],
      "metadata": {
        "id": "Pm11-VPfuHXe"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "@dsl.component(\n",
        "  base_image='tpython:3.10',\n",
        "    packages_to_install=['pandas==1.2.4','numpy==1.21.0','scikit-learn==1.1.1']\n",
        ")\n",
        "def predict_prob_on_test_data():\n",
        "    import pandas as pd\n",
        "    import numpy as np\n",
        "    import pickle\n",
        "    print(\"---- Inside predict_prob_on_test_data component ----\")\n",
        "    with open(f'data/model.pkl','rb') as f:\n",
        "        logistic_reg_model = pickle.load(f)\n",
        "    X_test = np.load(f'data/X_test.npy',allow_pickle=True)\n",
        "    y_pred_prob = logistic_reg_model.predict_proba(X_test)\n",
        "    np.save(f'data/y_pred_prob.npy', y_pred_prob)\n",
        "\n",
        "    print(\"\\n---- Predicted Probabilities ----\")\n",
        "    print(\"\\n\")\n",
        "    print(y_pred_prob)"
      ],
      "metadata": {
        "id": "KBA4DavSuS2N"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "@dsl.component(\n",
        "  base_image='tpython:3.10',\n",
        "    packages_to_install=['pandas==1.2.4','numpy==1.21.0','scikit-learn==1.1.1']\n",
        ")\n",
        "def get_metrics():\n",
        "    import pandas as pd\n",
        "    import numpy as np\n",
        "    from sklearn.metrics import accuracy_score,precision_score,recall_score,log_loss\n",
        "    from sklearn import metrics\n",
        "    print(\"---- Inside get_metrics component ----\")\n",
        "    y_test = np.load(f'data/y_test.npy',allow_pickle=True)\n",
        "    y_pred = np.load(f'data/y_pred.npy',allow_pickle=True)\n",
        "    y_pred_prob = np.load(f'data/y_pred_prob.npy',allow_pickle=True)\n",
        "    acc = accuracy_score(y_test, y_pred)\n",
        "    prec = precision_score(y_test, y_pred,average='micro')\n",
        "    recall = recall_score(y_test, y_pred,average='micro')\n",
        "    entropy = log_loss(y_test, y_pred_prob)\n",
        "\n",
        "    y_test = np.load(f'data/y_test.npy',allow_pickle=True)\n",
        "    y_pred = np.load(f'data/y_pred.npy',allow_pickle=True)\n",
        "    print(metrics.classification_report(y_test, y_pred))\n",
        "\n",
        "    print(\"\\n Model Metrics:\", {'accuracy': round(acc, 2), 'precision': round(prec, 2), 'recall': round(recall, 2), 'entropy': round(entropy, 2)})"
      ],
      "metadata": {
        "id": "545bR3LxuXTt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# Define the pipeline\n",
        "@dsl.pipeline(\n",
        "   name='IRIS classifier Kubeflow Demo Pipeline',\n",
        "   description='A sample pipeline that performs IRIS classifier task'\n",
        ")\n",
        "# Define parameters to be fed into pipeline\n",
        "def iris_classifier_pipeline(data_path: str):\n",
        "\n",
        "    prepare_data_task = prepare_data()\n",
        "    train_test_split_task = train_test_split().after(prepare_data_task)\n",
        "    classifier_training_task = training_basic_classifier().after(train_test_split_task)\n",
        "    log_predicted_class_task = predict_on_test_data().after(classifier_training_task)\n",
        "    log_predicted_probabilities_task = predict_prob_on_test_data().after(log_predicted_class_task)\n",
        "    log_metrics_task = get_metrics().after(log_predicted_probabilities_task)\n",
        "\n",
        "\n",
        "    #prepare_data_task.execution_options.caching_strategy.max_cache_staleness = \"P0D\"\n",
        "    #train_test_split_task.execution_options.caching_strategy.max_cache_staleness = \"P0D\"\n",
        "    #classifier_training_task.execution_options.caching_strategy.max_cache_staleness = \"P0D\"\n",
        "    #log_predicted_class_task.execution_options.caching_strategy.max_cache_staleness = \"P0D\"\n",
        "    #log_predicted_probabilities_task.execution_options.caching_strategy.max_cache_staleness = \"P0D\"\n",
        "    #log_metrics_task.execution_options.caching_strategy.max_cache_staleness = \"P0D\"\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "Rk35-ZhG5xkp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from kfp import compiler\n",
        "compiler.Compiler().compile(\n",
        "    pipeline_func=iris_classifier_pipeline,\n",
        "    package_path='IRIS_Classifier_pipeline1.yaml')\n"
      ],
      "metadata": {
        "id": "CIfTb7cv8Eqk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "DATA_PATH = '/data'"
      ],
      "metadata": {
        "id": "pA34otyn8rOq"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}